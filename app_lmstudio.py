"""
RAGãƒ™ãƒ¼ã‚¹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ - LM Studioç‰ˆ
LangChainã€FAISSã€Sentence Transformersã€LM Studio APIã‚’ä½¿ç”¨
"""

import os
import glob
from typing import List, Tuple, Optional
import gradio as gr
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader
import pdfplumber
from langchain_core.documents import Document
from openai import OpenAI
import ipaddress
import time

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§retrieverã¨LM Studio clientã‚’ä¿æŒ
retriever = None
lm_studio_client = None
document_summary = None  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¦‚è¦ã‚’ä¿æŒ
all_documents_text = ""  # å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆ

# LM Studioè¨­å®š
LM_STUDIO_BASE_URL = "http://localhost:1234/v1"  # LM Studioã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
LM_STUDIO_API_KEY = "lm-studio"  # ãƒ€ãƒŸãƒ¼ï¼ˆLM Studioã§ã¯ä¸è¦ï¼‰

# ========================================
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
# ========================================

# IPåˆ¶é™ã®æœ‰åŠ¹/ç„¡åŠ¹ï¼ˆé–‹ç™ºæ™‚ã¯Falseã«ã™ã‚‹ã¨ä¾¿åˆ©ï¼‰
ENABLE_IP_RESTRICTION = False  # True = IPåˆ¶é™æœ‰åŠ¹ã€False = åˆ¶é™ãªã—

# è¨±å¯ã™ã‚‹IPã‚¢ãƒ‰ãƒ¬ã‚¹ç¯„å›²ï¼ˆãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¨èˆ¬ï¼‰
ALLOWED_IP_RANGES = [
    "127.0.0.0/8",      # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ›ã‚¹ãƒˆ
    "10.0.0.0/8",       # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆã‚¯ãƒ©ã‚¹Aï¼‰
    "172.16.0.0/12",    # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆã‚¯ãƒ©ã‚¹Bï¼‰
    "192.168.0.0/16",   # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆã‚¯ãƒ©ã‚¹C - è‡ªå®…ç’°å¢ƒã§ä¸€èˆ¬çš„ï¼‰
    # ç¤¾å†…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãªã©ç‰¹å®šã®IPã®ã¿ã«åˆ¶é™ã—ãŸã„å ´åˆã¯ã€ä¸Šè¨˜ã‚’å‰Šé™¤ã—ã¦è¨­å®š
]

# èªè¨¼æƒ…å ±ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼åã¨ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ï¼‰
# æœ¬ç•ªç’°å¢ƒã§ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€ã“ã¨ã‚’æ¨å¥¨
VALID_USERS = {
    "admin": "password123",      # ç®¡ç†è€…
    "user1": "secure_pass",      # ãƒ¦ãƒ¼ã‚¶ãƒ¼1
    "tanaka": "tanaka2024",      # ç”°ä¸­ã•ã‚“
    "suzuki": "suzuki2024",      # éˆ´æœ¨ã•ã‚“
    "sato": "sato2024",          # ä½è—¤ã•ã‚“
    # å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
}

def check_ip_allowed(request: gr.Request) -> bool:
    """
    ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®IPã‚¢ãƒ‰ãƒ¬ã‚¹ãŒè¨±å¯ç¯„å›²å†…ã‹ãƒã‚§ãƒƒã‚¯
    
    Args:
        request: Gradioãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    
    Returns:
        è¨±å¯ã•ã‚Œã¦ã„ã‚‹å ´åˆTrueã€ãã†ã§ãªã„å ´åˆFalse
    """
    # IPåˆ¶é™ãŒç„¡åŠ¹ãªå ´åˆã¯å¸¸ã«è¨±å¯
    if not ENABLE_IP_RESTRICTION:
        return True
    
    try:
        client_ip = request.client.host
        client_ip_obj = ipaddress.ip_address(client_ip)
        
        for ip_range in ALLOWED_IP_RANGES:
            if client_ip_obj in ipaddress.ip_network(ip_range):
                print(f"[OK] Access allowed: {client_ip}")
                return True
        
        print(f"[WARNING] Access denied: {client_ip} (not in allowed range)")
        return False
    except Exception as e:
        print(f"[ERROR] IP check error: {e}")
        return False

def authenticate_user(username: str, password: str) -> bool:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼èªè¨¼
    
    Args:
        username: ãƒ¦ãƒ¼ã‚¶ãƒ¼å
        password: ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰
    
    Returns:
        èªè¨¼æˆåŠŸã®å ´åˆTrue
    """
    if username in VALID_USERS and VALID_USERS[username] == password:
        print(f"[OK] User authenticated: {username}")
        return True
    print(f"[WARNING] Authentication failed: {username}")
    return False

def generate_document_summary(documents: List, client) -> str:
    """
    å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ¦‚è¦ã‚’LLMã§ç”Ÿæˆ
    
    Args:
        documents: èª­ã¿è¾¼ã¾ã‚ŒãŸæ–‡æ›¸ã®ãƒªã‚¹ãƒˆ
        client: LM Studio client
    
    Returns:
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ¦‚è¦ãƒ†ã‚­ã‚¹ãƒˆ
    """
    if not client or not documents:
        return None
    
    print("[INFO] Generating document summary with LLM...")
    
    try:
        # å…¨æ–‡æ›¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚çŸ­ã‚ã«åˆ¶é™ï¼‰
        # æ—¥æœ¬èªã¯ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¤šããªã‚ŠãŒã¡ãªã®ã§ã€ç´„3000æ–‡å­—ç¨‹åº¦ã«åˆ¶é™
        combined_text = ""
        max_chars = 3000
        
        # å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰å°‘ã—ãšã¤æŠ½å‡ºã—ã¦å¤šæ§˜æ€§ã‚’ç¢ºä¿
        chars_per_doc = max(100, max_chars // len(documents))
        
        for doc in documents:
            content = doc.page_content[:chars_per_doc]
            combined_text += content + "\n\n"
            if len(combined_text) > max_chars:
                combined_text = combined_text[:max_chars] + "..."
                break
        
        # LLMã«æ¦‚è¦ç”Ÿæˆã‚’ä¾é ¼
        prompt = f"""ä»¥ä¸‹ã®æŠ€è¡“æ–‡æ›¸ã®å†…å®¹ï¼ˆæŠœç²‹ï¼‰ã‚’åˆ†æã—ã¦ã€åŒ…æ‹¬çš„ãªæ¦‚è¦ã‚’æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€æ–‡æ›¸å†…å®¹ï¼ˆæŠœç²‹ï¼‰ã€‘
{combined_text}

ã€å‡ºåŠ›å½¢å¼ã€‘
ä»¥ä¸‹ã®é …ç›®ã‚’å«ã‚ã¦ã€è¦‹ã‚„ã™ãæ•´ç†ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
1. ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯ï¼ˆç®‡æ¡æ›¸ãï¼‰
2. æ–‡æ›¸ã®ç›®çš„ã¨å¯¾è±¡èª­è€…
3. é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç®‡æ¡æ›¸ãï¼‰
4. æ¨å¥¨ã•ã‚Œã‚‹è³ªå•ä¾‹ï¼ˆ3-5å€‹ï¼‰

ã€æ¦‚è¦ã€‘"""
        
        response = client.chat.completions.create(
            model="local-model",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=800,
        )
        
        summary = response.choices[0].message.content
        print("[OK] Document summary generated successfully!")
        return summary
        
    except Exception as e:
        print(f"[WARNING] Could not generate summary: {e}")
        return None

def load_documents(documents_path: str = "./documents") -> List:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰æŠ€è¡“æ–‡æ›¸ã‚’èª­ã¿è¾¼ã‚€
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã—ã€å¤‰æ›´ãŒã‚ã‚Œã°å†èª­ã¿è¾¼ã¿ã™ã‚‹
    
    Args:
        documents_path: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
    
    Returns:
        èª­ã¿è¾¼ã¾ã‚ŒãŸæ–‡æ›¸ã®ãƒªã‚¹ãƒˆ
    """
    import pickle
    
    print(f"[1/4] Loading documents from: {documents_path}")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™
    cache_dir = "./vectorstore_cache"
    os.makedirs(cache_dir, exist_ok=True)
    docs_cache_path = os.path.join(cache_dir, "documents_cache.pkl")
    
    # ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ…‹ã‚’å–å¾—ï¼ˆãƒ‘ã‚¹ã¨æ›´æ–°æ—¥æ™‚ï¼‰
    current_files_state = {}
    
    # å¯¾è±¡ã¨ã™ã‚‹æ‹¡å¼µå­
    target_extensions = ["*.txt", "*.pdf", "*.docx"]
    all_files = []
    
    for ext in target_extensions:
        all_files.extend(glob.glob(os.path.join(documents_path, ext)))
        
    for file_path in all_files:
        try:
            # çµ¶å¯¾ãƒ‘ã‚¹ã§ã‚­ãƒ¼ã‚’ä½œæˆã—ã€mtimeã‚’ä¿å­˜
            abs_path = os.path.abspath(file_path)
            mtime = os.path.getmtime(file_path)
            current_files_state[abs_path] = mtime
        except Exception:
            pass
            
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç¢ºèª
    if os.path.exists(docs_cache_path):
        try:
            print("  Checking document cache...")
            with open(docs_cache_path, 'rb') as f:
                cached_data = pickle.load(f)
            
            cached_state = cached_data.get('file_state', {})
            cached_documents = cached_data.get('documents', [])
            
            # çŠ¶æ…‹ã®æ¯”è¼ƒ
            # 1. ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒåŒã˜ã‹
            # 2. ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚ãŒåŒã˜ã‹
            is_cache_valid = True
            
            if len(current_files_state) != len(cached_state):
                print("  [INFO] File count changed, reloading...")
                is_cache_valid = False
            else:
                for path, mtime in current_files_state.items():
                    if path not in cached_state or cached_state[path] != mtime:
                        print(f"  [INFO] File changed: {os.path.basename(path)}")
                        is_cache_valid = False
                        break
            
            if is_cache_valid and cached_documents:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚“ã ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ã«ã™ã‚‹
                cleaned_documents = []
                for doc in cached_documents:
                    # æ—¢ã«Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
                    if isinstance(doc, Document):
                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                        cleaned_metadata = {}
                        for key, value in doc.metadata.items():
                            # ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå‹ã®ã¿ã‚’ä¿æŒ
                            if isinstance(value, (str, int, float, bool, type(None))):
                                cleaned_metadata[key] = value
                            elif isinstance(value, (list, tuple)):
                                # ãƒªã‚¹ãƒˆã‚„ã‚¿ãƒ—ãƒ«ã¯æ–‡å­—åˆ—ã«å¤‰æ›
                                cleaned_metadata[key] = str(value)
                            else:
                                # ãã®ä»–ã¯æ–‡å­—åˆ—ã«å¤‰æ›
                                cleaned_metadata[key] = str(value)
                        doc.metadata = cleaned_metadata
                        cleaned_documents.append(doc)
                    else:
                        # è¾æ›¸å½¢å¼ã®å ´åˆã¯Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                        if isinstance(doc, dict):
                            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                            cleaned_metadata = {}
                            metadata = doc.get('metadata', {})
                            for key, value in metadata.items():
                                if isinstance(value, (str, int, float, bool, type(None))):
                                    cleaned_metadata[key] = value
                                else:
                                    cleaned_metadata[key] = str(value)
                            
                            cleaned_doc = Document(
                                page_content=doc.get('page_content', ''),
                                metadata=cleaned_metadata
                            )
                            cleaned_documents.append(cleaned_doc)
                        else:
                            cleaned_documents.append(doc)
                
                print(f"  [OK] Loaded {len(cleaned_documents)} documents from cache (Fast Load)")
                return cleaned_documents
                
        except Exception as e:
            print(f"  [WARNING] Cache load failed: {e}")
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿å¤±æ•—æ™‚ã¯é€šå¸¸èª­ã¿è¾¼ã¿ã¸
            
    # ã“ã“ã‹ã‚‰é€šå¸¸èª­ã¿è¾¼ã¿ï¼ˆå¤‰æ›´ãŒã‚ã£ãŸå ´åˆã‚„åˆå›ï¼‰
    print("  Reloading all documents...")
    documents = []
    
    # TXTãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æŒ‡å®šï¼‰
    txt_files = glob.glob(os.path.join(documents_path, "*.txt"))
    for file_path in txt_files:
        try:
            print(f"  - Loading: {os.path.basename(file_path)}")
            loader = TextLoader(file_path, encoding='utf-8')
            documents.extend(loader.load())
        except Exception as e:
            print(f"  [WARNING] Failed to load {file_path}: {e}")
    
    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã§å®‰å…¨ã«å®Ÿè¡Œï¼‰
    import json
    import subprocess
    import sys

    pdf_files = glob.glob(os.path.join(documents_path, "*.pdf"))
    total_pdfs = len(pdf_files)
    
    if total_pdfs > 0:
        print(f"  Found {total_pdfs} PDF files")

    # ä»¥å‰ã®ã‚¹ã‚­ãƒƒãƒ—ãƒªã‚¹ãƒˆã¯ä¸è¦ã«ãªã£ãŸãŸã‚å‰Šé™¤ã—ã¾ã—ãŸ
    # ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹åŒ–ã«ã‚ˆã‚Šã€ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¯è‡ªå‹•çš„ã«ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™

    for idx, file_path in enumerate(pdf_files, 1):
        basename = os.path.basename(file_path)
        print(f"  [{idx}/{total_pdfs}] Loading: {basename}")
        
        try:
            # pdf_worker.py ã‚’ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã¨ã—ã¦å®Ÿè¡Œ
            # ã“ã‚Œã«ã‚ˆã‚Šã€PDFèª­ã¿è¾¼ã¿ä¸­ã«ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ã¦ã‚‚ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¯å®ˆã‚‰ã‚Œã‚‹
            result = subprocess.run(
                [sys.executable, "pdf_worker.py", file_path],
                capture_output=True,
                text=True,
                encoding='utf-8',
                timeout=60  # 60ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            )
            
            if result.returncode == 0:
                # æˆåŠŸã—ãŸå ´åˆã€JSONå‡ºåŠ›ã‚’ãƒ‘ãƒ¼ã‚¹
                try:
                    loaded_docs_data = json.loads(result.stdout)
                    
                    # è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                    loaded_count = 0
                    for doc_data in loaded_docs_data:
                        doc = Document(
                            page_content=doc_data['page_content'],
                            metadata=doc_data['metadata']
                        )
                        documents.append(doc)
                        loaded_count += 1
                        
                    loader_name = loaded_docs_data[0]['metadata'].get('loader', 'unknown')
                    print(f"    âœ“ {loaded_count} pages loaded ({loader_name})")
                    
                except json.JSONDecodeError:
                    print(f"    [WARNING] Failed to parse worker output for {basename}")
            else:
                # å¤±æ•—ã—ãŸå ´åˆï¼ˆã‚¯ãƒ©ãƒƒã‚·ãƒ¥å«ã‚€ï¼‰
                error_msg = result.stderr.strip()
                if not error_msg:
                    error_msg = "Process crashed or returned no output"
                
                print(f"    [WARNING] Failed to load {basename} (skipped)")
                # ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                # print(f"      Error: {error_msg[:200]}")
                
        except subprocess.TimeoutExpired:
            print(f"    [WARNING] Timeout while loading {basename} (skipped)")
        except Exception as e:
            print(f"    [ERROR] Unexpected error executing worker: {e}")
            
    # DOCXãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    docx_files = glob.glob(os.path.join(documents_path, "*.docx"))
    for file_path in docx_files:
        try:
            print(f"  - Loading: {os.path.basename(file_path)}")
            loader = Docx2txtLoader(file_path)
            documents.extend(loader.load())
        except Exception as e:
            print(f"  [WARNING] Failed to load {file_path}: {e}")
    
    # èª­ã¿è¾¼ã¿å®Œäº†å¾Œã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    if documents:
        try:
            print("  Saving document cache...")
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦ã‹ã‚‰ä¿å­˜
            cleaned_documents_for_cache = []
            for doc in documents:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                cleaned_metadata = {}
                for key, value in doc.metadata.items():
                    # ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå‹ã®ã¿ã‚’ä¿æŒ
                    if isinstance(value, (str, int, float, bool, type(None))):
                        cleaned_metadata[key] = value
                    elif isinstance(value, (list, tuple)):
                        # ãƒªã‚¹ãƒˆã‚„ã‚¿ãƒ—ãƒ«ã¯æ–‡å­—åˆ—ã«å¤‰æ›
                        cleaned_metadata[key] = str(value)
                    else:
                        # ãã®ä»–ã¯æ–‡å­—åˆ—ã«å¤‰æ›
                        cleaned_metadata[key] = str(value)
                
                # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§æ–°ã—ã„Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                cleaned_doc = Document(
                    page_content=doc.page_content,
                    metadata=cleaned_metadata
                )
                cleaned_documents_for_cache.append(cleaned_doc)
            
            cache_data = {
                'file_state': current_files_state,
                'documents': cleaned_documents_for_cache
            }
            with open(docs_cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            print("  [OK] Document cache saved")
        except Exception as e:
            print(f"  [WARNING] Failed to save document cache: {e}")
            
    print(f"[OK] Loaded {len(documents)} documents")
    return documents

def create_vector_store(documents: List):
    """
    æ–‡æ›¸ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã€åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã—ã¦FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
    æ—¢ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãã‚Œã‚’å†åˆ©ç”¨
    
    Args:
        documents: èª­ã¿è¾¼ã¾ã‚ŒãŸæ–‡æ›¸ã®ãƒªã‚¹ãƒˆ
    
    Returns:
        FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢
    """
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    cache_dir = "./vectorstore_cache"
    faiss_index_path = os.path.join(cache_dir, "faiss.index")
    pkl_path = os.path.join(cache_dir, "index.pkl")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯èª­ã¿è¾¼ã‚€
    if os.path.exists(faiss_index_path) and os.path.exists(pkl_path):
        print("[INFO] Found cached vector store, loading...")
        try:
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
            )
            vector_store = FAISS.load_local(
                cache_dir,
                embeddings,
                allow_dangerous_deserialization=True
            )
            print("[OK] Loaded cached vector store")
            return vector_store
        except Exception as e:
            print(f"[WARNING] Failed to load cache: {e}")
            print("[INFO] Rebuilding vector store...")
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå£Šã‚Œã¦ã„ã‚‹å ´åˆã¯å‰Šé™¤
            try:
                os.remove(faiss_index_path)
                os.remove(pkl_path)
            except:
                pass
    
    print("[2/4] Splitting documents into chunks...")
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’é‡è¤‡ã‚’æŒãŸã›ãªãŒã‚‰åˆ†å‰²ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¿æŒã®ãŸã‚ï¼‰
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # å„ãƒãƒ£ãƒ³ã‚¯ã®æ–‡å­—æ•°
        chunk_overlap=50,  # ãƒãƒ£ãƒ³ã‚¯é–“ã®é‡è¤‡æ–‡å­—æ•°
        length_function=len,
    )
    chunks = text_splitter.split_documents(documents)
    
    # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚„Noneã‚’å«ã‚€ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_chunks = []
    for chunk in chunks:
        # æ–‡å­—åˆ—å‹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        if (chunk.page_content and 
            isinstance(chunk.page_content, str) and 
            chunk.page_content.strip()):
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆåˆ¶å¾¡æ–‡å­—ã‚„ä¸æ­£ãªæ–‡å­—ã‚’é™¤å»ï¼‰
            cleaned_text = chunk.page_content.strip()
            # Noneã‚„ç©ºæ–‡å­—åˆ—ã§ãªã„ã“ã¨ã‚’ç¢ºèª
            if cleaned_text and len(cleaned_text) > 0:
                chunk.page_content = cleaned_text
                filtered_chunks.append(chunk)
    
    original_count = len(chunks)
    chunks = filtered_chunks
    print(f"[OK] Created {len(chunks)} chunks (filtered from {original_count} total)")
    
    if not chunks:
        raise ValueError("No valid chunks found after filtering. Please check your documents.")
    
    print("[2/4] Loading embedding model...")
    # sentence-transformersã‚’ä½¿ç”¨ã—ãŸåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
    )
    
    print("[3/4] Creating FAISS vector store...")
    # FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆï¼ˆãƒãƒƒãƒå‡¦ç†ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å‘ä¸Šï¼‰
    try:
        # å¤§é‡ã®ãƒãƒ£ãƒ³ã‚¯ãŒã‚ã‚‹å ´åˆã€ãƒãƒƒãƒå‡¦ç†ã§åˆ†å‰²
        batch_size = 1000
        if len(chunks) > batch_size:
            print(f"  Processing {len(chunks)} chunks in batches of {batch_size}...")
            
            # ãƒãƒƒãƒã‚’æ¤œè¨¼ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹é–¢æ•°
            def validate_batch(batch_chunks):
                """ãƒãƒƒãƒå†…ã®ãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œè¨¼ã—ã€æœ‰åŠ¹ãªã‚‚ã®ã ã‘ã‚’è¿”ã™"""
                valid_chunks = []
                for chunk in batch_chunks:
                    # ãƒ†ã‚­ã‚¹ãƒˆãŒæ–‡å­—åˆ—å‹ã§ã€ç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
                    if (chunk.page_content and 
                        isinstance(chunk.page_content, str) and 
                        chunk.page_content.strip() and
                        len(chunk.page_content.strip()) > 0):
                        # åˆ¶å¾¡æ–‡å­—ã‚„ä¸æ­£ãªæ–‡å­—ã‚’é™¤å»
                        cleaned = chunk.page_content.strip()
                        # Noneã‚„ç©ºæ–‡å­—åˆ—ã§ãªã„ã“ã¨ã‚’å†ç¢ºèª
                        if cleaned:
                            chunk.page_content = cleaned
                            valid_chunks.append(chunk)
                return valid_chunks
            
            # æœ€åˆã®ãƒãƒƒãƒã§ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
            first_batch = validate_batch(chunks[:batch_size])
            if not first_batch:
                raise ValueError("No valid chunks in first batch")
            vector_store = FAISS.from_documents(first_batch, embeddings)
            
            # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’è¿½åŠ 
            for i in range(batch_size, len(chunks), batch_size):
                batch = chunks[i:i+batch_size]
                # ãƒãƒƒãƒã‚’æ¤œè¨¼ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                valid_batch = validate_batch(batch)
                if valid_batch:
                    try:
                        vector_store.add_documents(valid_batch)
                        print(f"  Processed {min(i+batch_size, len(chunks))}/{len(chunks)} chunks...")
                    except Exception as batch_error:
                        print(f"  [WARNING] Error processing batch {i//batch_size + 1}: {str(batch_error)[:100]}")
                        print(f"  Skipping this batch and continuing...")
                        continue
                else:
                    print(f"  [WARNING] Batch {i//batch_size + 1} had no valid chunks, skipping...")
        else:
            vector_store = FAISS.from_documents(chunks, embeddings)
        print("[OK] Vector store created")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦ä¿å­˜
        print("[INFO] Saving vector store to cache...")
        try:
            os.makedirs(cache_dir, exist_ok=True)
            vector_store.save_local(cache_dir)
            print("[OK] Vector store cached for future use")
        except Exception as cache_error:
            print(f"[WARNING] Failed to save cache: {cache_error}")
            print("  Vector store will be rebuilt on next run")
    except Exception as e:
        print(f"[ERROR] Failed to create vector store: {e}")
        print(f"  Number of chunks: {len(chunks)}")
        print(f"  Sample chunk content (first 100 chars): {chunks[0].page_content[:100] if chunks else 'N/A'}")
        raise
    
    return vector_store

def initialize_lm_studio():
    """
    LM Studio APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
    
    Returns:
        OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    print("[4/4] Connecting to LM Studio...")
    print(f"  Base URL: {LM_STUDIO_BASE_URL}")
    
    try:
        client = OpenAI(
            base_url=LM_STUDIO_BASE_URL,
            api_key=LM_STUDIO_API_KEY
        )
        
        # æ¥ç¶šãƒ†ã‚¹ãƒˆ
        models = client.models.list()
        model_names = [model.id for model in models.data]
        
        if model_names:
            print(f"[OK] Connected to LM Studio")
            print(f"  Available models: {', '.join(model_names)}")
            return client
        else:
            print("[WARNING] No models loaded in LM Studio")
            print("  Please load a model in LM Studio and restart")
            return None
            
    except Exception as e:
        print(f"[ERROR] Failed to connect to LM Studio: {e}")
        print("  Make sure LM Studio is running and server is started")
        return None

def setup_qa_chain(documents_path: str = "./documents"):
    """
    RAG QAãƒã‚§ãƒ¼ãƒ³å…¨ä½“ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    
    Args:
        documents_path: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
    
    Returns:
        æˆåŠŸæ™‚ã¯Trueã€å¤±æ•—æ™‚ã¯False
    """
    global retriever, lm_studio_client, document_summary, all_documents_text
    
    print("=" * 60)
    print("RAG Search System - LM Studio Edition")
    print("=" * 60)
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€
    try:
        documents = load_documents(documents_path)
    except KeyboardInterrupt:
        print("\n[INFO] Document loading interrupted by user")
        raise
    except Exception as e:
        print(f"\n[ERROR] Failed to load documents: {e}")
        import traceback
        print(traceback.format_exc())
        return False
    
    if not documents:
        print("[WARNING] No documents found")
        print(f"  Place .txt, .pdf, or .docx files in {documents_path} folder")
        return False
    
    # å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜
    all_documents_text = "\n\n".join([doc.page_content for doc in documents])
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
    vector_store = create_vector_store(documents)
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: Retrieverã‚’ä½œæˆ
    retriever = vector_store.as_retriever(
        search_kwargs={"k": 5}  # ä¸Šä½5ã¤ã®é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ï¼ˆã‚ˆã‚Šå¤šãã®æƒ…å ±ï¼‰
    )
    
    # ã‚¹ãƒ†ãƒƒãƒ—4: LM Studioã«æ¥ç¶š
    lm_studio_client = initialize_lm_studio()
    
    # ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¦‚è¦ã‚’ç”Ÿæˆï¼ˆLM StudioãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
    if lm_studio_client:
        document_summary = generate_document_summary(documents, lm_studio_client)
    
    print("=" * 60)
    print("[OK] Initialization complete!")
    print("=" * 60)
    
    return True

def chat(message: str, history: List) -> str:
    """
    ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ï¼ˆLM Studioç‰ˆï¼‰
    
    Args:
        message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•
        history: ãƒãƒ£ãƒƒãƒˆå±¥æ­´ï¼ˆGradioãŒè‡ªå‹•ç®¡ç†ï¼‰
    
    Returns:
        ãƒœãƒƒãƒˆã‹ã‚‰ã®å›ç­”
    """
    global retriever, lm_studio_client, document_summary
    
    if retriever is None:
        return "[ERROR] Chatbot not initialized. Please restart the app."
    
    # ç‰¹æ®Šã‚³ãƒãƒ³ãƒ‰ã®å‡¦ç†
    if message.strip() in ["æ¦‚è¦", "ã‚µãƒãƒªãƒ¼", "summary", "/æ¦‚è¦", "/summary"]:
        if document_summary:
            return f"# ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¦‚è¦\n\n{document_summary}"
        else:
            return "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¦‚è¦ã¯ã¾ã ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚LM StudioãŒæ¥ç¶šã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
    
    # è‡ªç”±ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ï¼ˆ/chat ã¾ãŸã¯ /talk ã§é–‹å§‹ï¼‰
    if message.strip().startswith(("/chat ", "/talk ", "é›‘è«‡ ", "ä¼šè©± ")):
        # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤
        for prefix in ["/chat ", "/talk ", "é›‘è«‡ ", "ä¼šè©± "]:
            if message.strip().startswith(prefix):
                actual_message = message.strip()[len(prefix):]
                break
        
        if lm_studio_client:
            try:
                response = lm_studio_client.chat.completions.create(
                    model="local-model",
                    messages=[{"role": "user", "content": actual_message}],
                    temperature=0.8,
                    max_tokens=512,
                )
                return f"**ğŸ’¬ è‡ªç”±ä¼šè©±ãƒ¢ãƒ¼ãƒ‰**\n\n{response.choices[0].message.content}"
            except Exception as e:
                return f"[ERROR] LM Studio error: {e}"
        else:
            return "è‡ªç”±ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ã†ã«ã¯ã€LM Studioã‚’èµ·å‹•ã—ã¦ã‚µãƒ¼ãƒãƒ¼ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚"
    
    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢
        docs = retriever.invoke(message)
        
        if not docs:
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: LM Studioã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
        if lm_studio_client is not None:
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆï¼ˆä¸Šä½5ä»¶ã‚’ä½¿ç”¨ï¼‰
            context = "\n\n".join([doc.page_content for doc in docs[:5]])
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆï¼ˆã‚³ãƒ³ã‚·ã‚§ãƒ«ã‚¸ãƒ¥çš„ãªãƒˆãƒ¼ãƒ³ï¼‰
            prompt = f"""ã‚ãªãŸã¯æŠ€è¡“æ–‡æ›¸æ¤œç´¢ã®ã‚³ãƒ³ã‚·ã‚§ãƒ«ã‚¸ãƒ¥ã§ã™ã€‚ãŠå®¢æ§˜ã®ã€Œãµã‚“ã‚ã‚Šã¨ã—ãŸè³ªå•ã€ã‹ã‚‰ã€é–¢é€£ã™ã‚‹æŠ€è¡“æ–‡æ›¸ã®å†…å®¹ã‚’è¦‹ã¤ã‘ã¦ã€ã‚ã‹ã‚Šã‚„ã™ãæ¡ˆå†…ã—ã¾ã™ã€‚

ä»¥ä¸‹ã®æ–‡æ›¸ã‹ã‚‰ã€ãŠå®¢æ§˜ã®è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’æ¢ã—ã¦ã€ä¸å¯§ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

ã€ãŠå®¢æ§˜ã‹ã‚‰ã®è³ªå•ã€‘
{message}

ã€é–¢é€£ã™ã‚‹æŠ€è¡“æ–‡æ›¸ã®å†…å®¹ã€‘
{context}

ã€ã”æ¡ˆå†…ã€‘
ä¸Šè¨˜ã®æ–‡æ›¸ã‚’ç¢ºèªã—ãŸã¨ã“ã‚ã€"""
            
            # LM Studioã§å›ç­”ã‚’ç”Ÿæˆ
            try:
                response = lm_studio_client.chat.completions.create(
                    model="local-model",  # LM Studioã¯è‡ªå‹•çš„ã«ãƒ­ãƒ¼ãƒ‰ä¸­ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                    messages=[
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=1024,  # ã‚ˆã‚Šè©³ã—ã„å›ç­”ã®ãŸã‚ã«å¢—åŠ 
                )
                
                ai_response = response.choices[0].message.content
                
                answer = f"## ğŸ’¬ ã”è³ªå•\n{message}\n\n"
                answer += f"## ğŸ” TechScoutã‹ã‚‰ã®ã”æ¡ˆå†…\n{ai_response}\n\n"
                
                # ã‚ˆã‚Šè©³ç´°ãªå‚ç…§å…ƒæƒ…å ±ã‚’è¿½åŠ 
                answer += "---\n\n"
                answer += "## ğŸ“š å‚ç…§ã—ãŸæ–‡æ›¸\n\n"
                seen_sources = {}
                for doc in docs[:5]:
                    source = doc.metadata.get('source', 'ä¸æ˜')
                    source_name = os.path.basename(source)
                    
                    if source_name not in seen_sources:
                        seen_sources[source_name] = []
                    
                    # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’å–å¾—ï¼ˆPDFã®å ´åˆï¼‰
                    page = doc.metadata.get('page', None)
                    preview = doc.page_content[:150].replace('\n', ' ')
                    seen_sources[source_name].append({
                        'page': page,
                        'preview': preview
                    })
                
                for source_name, chunks in seen_sources.items():
                    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒªãƒ³ã‚¯ã‚’ä½œæˆ
                    if source_name.endswith('.pdf'):
                        # ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’å–å¾—
                        source_path = None
                        for doc in docs[:5]:
                            if os.path.basename(doc.metadata.get('source', '')) == source_name:
                                source_path = doc.metadata.get('source', '')
                                break
                        
                        if source_path and os.path.exists(source_path):
                            # Gradioã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µãƒ¼ãƒãƒ¼çµŒç”±ã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªURLã‚’ä½œæˆ
                            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒªãƒ³ã‚¯ã¨ã—ã¦è¡¨ç¤º
                            # Windowsãƒ‘ã‚¹ã®å ´åˆã¯æ­£è¦åŒ–ãŒå¿…è¦
                            normalized_path = source_path.replace('\\', '/')
                            file_url = f"/file={normalized_path}"
                            answer += f"### ğŸ“„ [{source_name}]({file_url})\n"
                            answer += f"ğŸ“ å ´æ‰€: `documents/{source_name}`\n"
                            answer += f"ğŸ’¡ [ğŸ“¥ PDFã‚’é–‹ã/ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰]({file_url})\n\n"
                        else:
                            answer += f"### ğŸ“„ {source_name}\n"
                            answer += f"ğŸ“ å ´æ‰€: `documents/{source_name}`\n\n"
                    else:
                        answer += f"### ğŸ“„ {source_name}\n"
                    
                    for idx, chunk_info in enumerate(chunks, 1):
                        if chunk_info['page'] is not None:
                            answer += f"- **ãƒšãƒ¼ã‚¸ {chunk_info['page'] + 1}**: {chunk_info['preview']}...\n"
                        else:
                            answer += f"- {chunk_info['preview']}...\n"
                    answer += "\n"
                
                return answer
                
            except Exception as llm_error:
                print(f"[ERROR] LM Studio error: {llm_error}")
                # LLMãŒå¤±æ•—ã—ãŸå ´åˆã¯æ¤œç´¢çµæœã‚’ç›´æ¥è¡¨ç¤º
                pass
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: LM Studioæœªæ¥ç¶šæ™‚ã¯æ¤œç´¢çµæœã‚’æ•´å½¢ã—ã¦è¿”ã™
        answer = f"## ğŸ’¬ ã”è³ªå•\n{message}\n\n"
        answer += f"## ğŸ” TechScoutãŒè¦‹ã¤ã‘ãŸé–¢é€£æƒ…å ±\n\n"
        
        if lm_studio_client is None:
            answer += "_ğŸ’¡ LM Studioã«æ¥ç¶šã™ã‚‹ã¨ã€AIãŒå†…å®¹ã‚’è¦ç´„ã—ã¦å›ç­”ã—ã¾ã™_\n\n"
        
        # ã‚ˆã‚Šè©³ã—ãè¡¨ç¤º
        for i, doc in enumerate(docs[:5], 1):
            source = doc.metadata.get('source', 'ä¸æ˜')
            source_name = os.path.basename(source)
            page = doc.metadata.get('page', None)
            
            # ãƒšãƒ¼ã‚¸æƒ…å ±ä»˜ãã§è¡¨ç¤º
            page_info = f" (ãƒšãƒ¼ã‚¸ {page + 1})" if page is not None else ""
            
            # PDFãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒªãƒ³ã‚¯ã‚’ä½œæˆ
            if source_name.endswith('.pdf') and os.path.exists(source):
                normalized_path = source.replace('\\', '/')
                file_url = f"/file={normalized_path}"
                answer += f"### ğŸ“„ é–¢é€£ç®‡æ‰€ {i}: [{source_name}]({file_url}){page_info}\n\n"
            else:
                answer += f"### ğŸ“„ é–¢é€£ç®‡æ‰€ {i}: {source_name}{page_info}\n\n"
            
            # å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’è¡¨ç¤º
            content = doc.page_content[:600]  # è¡¨ç¤ºã‚’å¢—ã‚„ã™
            if len(doc.page_content) > 600:
                content += "..."
            
            answer += f"{content}\n\n"
        
        # ã‚½ãƒ¼ã‚¹æ–‡æ›¸ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æƒ…å ±ã‚’è¿½åŠ 
        answer += "---\n\n"
        answer += "## ğŸ“ æ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€\n\n"
        seen_sources = set()
        for doc in docs:
            source = doc.metadata.get('source', 'ä¸æ˜')
            source_name = os.path.basename(source)
            if source_name not in seen_sources:
                source_path = os.path.abspath(source)
                
                # PDFãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒªãƒ³ã‚¯ã‚’ä½œæˆ
                if source_name.endswith('.pdf') and os.path.exists(source):
                    normalized_path = source.replace('\\', '/')
                    file_url = f"/file={normalized_path}"
                    answer += f"- **[{source_name}]({file_url})**\n"
                    answer += f"  ğŸ“ `{source_path}`\n"
                    answer += f"  ğŸ’¡ [ğŸ“¥ PDFã‚’é–‹ã/ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰]({file_url})\n"
                else:
                    answer += f"- **{source_name}**\n"
                    answer += f"  ğŸ“ `{source_path}`\n"
                answer += "\n"
                seen_sources.add(source_name)
        
        return answer
    
    except Exception as e:
        return f"[ERROR] An error occurred: {str(e)}"

def main():
    """
    Gradio UIã‚’èµ·å‹•ã—ã¦ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’å®Ÿè¡Œ
    """
    # documentsãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
    if not os.path.exists("./documents"):
        os.makedirs("./documents")
        print("[INFO] Created 'documents' folder")
        print("  Place your technical documents here")
    
    # QAãƒã‚§ãƒ¼ãƒ³ã‚’åˆæœŸåŒ–ï¼ˆLM Studioç‰ˆï¼‰
    setup_qa_chain("./documents")
    
    # Gradioãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆ
    with gr.Blocks(title="TechScout - RAG Document Search") as demo:
        gr.Markdown(
            """
            # ğŸ” TechScout
            ### RAGæŠ€è¡“æ–‡æ›¸æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
            
            <div style="font-size: 0.85em; color: #666; margin-top: -15px; margin-bottom: 20px;">
            2025å¹´11æœˆ14æ—¥<br>
            æ±æ´‹é›»æ©Ÿè£½é€ æ ªå¼ä¼šç¤¾<br>
            é–‹ç™ºã‚»ãƒ³ã‚¿ãƒ¼åŸºç›¤æŠ€è¡“éƒ¨
            </div>
            
            æŠ€è¡“æ–‡æ›¸ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„ã€‚**LM Studioä¸Šã®LLM**ãŒé«˜å“è³ªãªå›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
            
            ## âœ¨ æ–°æ©Ÿèƒ½
            - ğŸ“š **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¦‚è¦è‡ªå‹•ç”Ÿæˆ** - èµ·å‹•æ™‚ã«å…¨æ–‡æ›¸ã®æ¦‚è¦ã‚’è‡ªå‹•ä½œæˆ
            - ğŸ’¬ **è‡ªç”±ä¼šè©±ãƒ¢ãƒ¼ãƒ‰** - LLMã¨ç›´æ¥ä¼šè©±ã§ãã¾ã™
            - ğŸ¯ **å¼·åŒ–ã•ã‚ŒãŸRAGæ¤œç´¢** - ã‚ˆã‚Šæ­£ç¢ºãªå›ç­”
            
            ## ğŸ® ä½¿ã„æ–¹
            
            ### RAGæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            æ™®é€šã«è³ªå•ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€æ–‡æ›¸ã‚’æ¤œç´¢ã—ã¦AIãŒå›ç­”ã—ã¾ã™ã€‚
            ```
            ä¾‹: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„
            ```
            
            ### ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¦‚è¦è¡¨ç¤º
            ```
            æ¦‚è¦
            ã¾ãŸã¯
            /summary
            ```
            
            ### ğŸ’¬ è‡ªç”±ä¼šè©±ãƒ¢ãƒ¼ãƒ‰
            ```
            /chat ã“ã‚“ã«ã¡ã¯
            é›‘è«‡ ä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿ
            ```
            
            ## ğŸ“ ã‚µãƒ³ãƒ—ãƒ«è³ªå•
            - **æ¦‚è¦ã‚’çŸ¥ã‚ŠãŸã„**: ã€Œæ¦‚è¦ã€ã¾ãŸã¯ã€Œã‚µãƒãƒªãƒ¼ã€
            - **ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**: ã€Œã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ã‚’æ•™ãˆã¦ãã ã•ã„ã€
            - **ãƒˆãƒ©ãƒ–ãƒ«å¯¾å¿œ**: ã€Œã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸæ™‚ã®å¯¾å‡¦æ³•ã¯ï¼Ÿã€
            - **ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ**: ã€Œã©ã®LLMãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†ã¹ãã§ã™ã‹ï¼Ÿã€
            - **è¨­å®šæ–¹æ³•**: ã€Œã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€
            - **è‡ªç”±ä¼šè©±**: ã€Œ/chat ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®ã‚³ãƒ„ã¯ï¼Ÿã€
            
            ## ğŸ“Š å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
            - `.txt` - ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
            - `.pdf` - PDFãƒ•ã‚¡ã‚¤ãƒ«
            - `.docx` - Wordæ–‡æ›¸
            
            ---
            _ğŸ’¡ ãƒ’ãƒ³ãƒˆ: LM StudioãŒèµ·å‹•ã—ã¦ã„ãªã„å ´åˆã¯æ¤œç´¢çµæœã®ã¿è¡¨ç¤ºã•ã‚Œã¾ã™_
            """
        )
        
        # ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
        chatbot = gr.ChatInterface(
            fn=chat,
            examples=[
                "æ¦‚è¦",
                "æ°¸ä¹…ç£çŸ³åŒæœŸãƒ¢ãƒ¼ã‚¿ã®æ¸©åº¦æ¨å®šæ–¹æ³•ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
                "èª˜å°é›»å‹•æ©Ÿã®ã‚»ãƒ³ã‚µãƒ¬ã‚¹åˆ¶å¾¡ã®åŸç†ã‚’èª¬æ˜ã—ã¦ãã ã•ã„",
                "ãƒ¢ãƒ¼ã‚¿åˆ¶å¾¡ã«ãŠã‘ã‚‹å›ç”Ÿé‹è»¢ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                "PMSMã¨IMï¼ˆèª˜å°æ©Ÿï¼‰ã®é•ã„ã«ã¤ã„ã¦",
                "ç£çŸ³æ¸©åº¦æ¨å®šã«ãŠã‘ã‚‹å¯¸æ³•ã°ã‚‰ã¤ãã®å½±éŸ¿ã¯ï¼Ÿ",
                "/chat ãƒ¢ãƒ¼ã‚¿åˆ¶å¾¡ã®å‹‰å¼·æ–¹æ³•ã‚’æ•™ãˆã¦",
            ],
            title="",
            description="",
            # themeå¼•æ•°ã¯ChatInterfaceã§ã¯ç„¡åŠ¹ï¼ˆBlocksã§è¨­å®šæ¸ˆã¿ï¼‰
        )
    
    # ã‚¢ãƒ—ãƒªã‚’èµ·å‹•
    print("\n" + "=" * 60)
    print("Starting Gradio app...")
    print("Security enabled:")
    print(f"  - IP restriction: {', '.join(ALLOWED_IP_RANGES)}")
    print(f"  - Authentication: Enabled ({len(VALID_USERS)} users)")
    print("=" * 60)
    
    try:
        # documentsãƒ•ã‚©ãƒ«ãƒ€ã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’å–å¾—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚µãƒ¼ãƒãƒ¼æ©Ÿèƒ½ç”¨ï¼‰
        documents_abs_path = os.path.abspath("./documents")
        
        demo.launch(
            share=False,  # å…¬é–‹ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆã—ãªã„
            server_name="0.0.0.0",  # LANå†…ã®å…¨ã¦ã®ãƒã‚·ãƒ³ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
            server_port=7861,  # ãƒãƒ¼ãƒˆç•ªå·ï¼ˆ7860ãŒä½¿ç”¨ä¸­ã®ãŸã‚7861ã«å¤‰æ›´ï¼‰
            auth=authenticate_user,  # ãƒ¦ãƒ¼ã‚¶ãƒ¼å/ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼
            auth_message="ğŸ” TechScout - æ±æ´‹é›»æ©Ÿè£½é€ æ ªå¼ä¼šç¤¾ é–‹ç™ºã‚»ãƒ³ã‚¿ãƒ¼åŸºç›¤æŠ€è¡“éƒ¨",
            allowed_paths=[documents_abs_path],  # PDFãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯
        )
    except KeyboardInterrupt:
        print("\n[INFO] Server stopped by user")
        return

if __name__ == "__main__":
    import sys
    try:
        main()
    except KeyboardInterrupt:
        print("\n[INFO] Application stopped by user")
        sys.exit(0)
    except Exception as e:
        print(f"\n[ERROR] Application crashed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

